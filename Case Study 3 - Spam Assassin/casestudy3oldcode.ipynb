# read file at /content/drive/MyDrive/SpamAssassinMessages.zip

import zipfile
import os

# Specify the path to your zip file
zip_file_path = '/content/drive/MyDrive/SpamAssassinMessages.zip'

# Create a ZipFile object
try:
  with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    # Extract all the contents of the zip file to a specified directory
    zip_ref.extractall('/content/extracted_files')  # You can specify a different directory if needed
    print(f"Successfully extracted files from {zip_file_path} to /content/extracted_files")
except FileNotFoundError:
  print(f"Error: File not found at {zip_file_path}")
except zipfile.BadZipFile:
  print(f"Error: Invalid zip file at {zip_file_path}")
except Exception as e:
  print(f"An unexpected error occurred: {e}")

# List extracted files to examine the dataset structure
extracted_files = os.listdir('/content/extracted_files')
"content/drive/MyDrive/extracted_files"

print(extracted_files[:10])  # Display first 10 files for inspection




import os

extract_path = '/content/extracted_files'

# Select a sample file from each category to inspect.  Handle cases where a directory might be missing.
sample_files = {}
for category in ["easy_ham", "spam", "hard_ham"]:
    category_path = os.path.join(extract_path, category)
    if os.path.exists(category_path) and os.listdir(category_path):
        sample_files[category] = os.listdir(category_path)[0]
    else:
        print(f"Warning: Directory '{category_path}' not found or empty. Skipping.")

# Read and display the first few lines of each file
sample_contents = {}
for category, file in sample_files.items():
    file_path = os.path.join(extract_path, category, file)
    try:
        with open(file_path, "r", encoding="latin-1") as f:
            sample_contents[category] = "".join(f.readlines()[:10])  # Read first 10 lines
    except FileNotFoundError:
        print(f"Warning: File '{file_path}' not found. Skipping.")
    except Exception as e:
        print(f"An error occurred while reading '{file_path}': {e}")

sample_contents



import re

# Function to extract email body from raw text
def extract_email_body(email_text):
    """
    Extracts the body of an email by removing headers.
    Assumes headers are separated from the body by a blank line.
    """
    split_email = email_text.split("\n\n", 1)  # Split at the first blank line
    return split_email[1] if len(split_email) > 1 else ""

# Extract sample email bodies
sample_email_bodies = {
    category: extract_email_body(content) for category, content in sample_contents.items()
}

sample_email_bodies





def clean_email_text(email_text):
    """
    Removes headers and retains only the email body.
    Assumes headers end at the first blank line.
    """
    lines = email_text.split("\n")
    body_lines = []
    in_body = False

    for line in lines:
        if in_body:
            body_lines.append(line)
        elif line.strip() == "":  # Empty line marks end of headers
            in_body = True

    return "\n".join(body_lines).strip()

# Extract cleaned email bodies
cleaned_email_bodies = {
    category: clean_email_text(content) for category, content in sample_contents.items()
}

cleaned_email_bodies


# Read a larger portion of each sample email to inspect content beyond headers
expanded_sample_contents = {}
for category, file in sample_files.items():
    file_path = os.path.join(extract_path, category, file)
    with open(file_path, "r", encoding="latin-1") as f:
        expanded_sample_contents[category] = "".join(f.readlines()[:50])  # Read first 50 lines

expanded_sample_contents


from bs4 import BeautifulSoup

def extract_email_body_refined(email_text):
    """
    Extracts the email body from raw email text.
    Removes headers and processes both plain text and HTML emails.
    """
    lines = email_text.split("\n")
    body_lines = []
    in_body = False

    for line in lines:
        # Identify start of body after metadata
        if in_body:
            body_lines.append(line)
        elif re.match(r"^Content-Type:.*", line, re.IGNORECASE):
            in_body = True

    email_body = "\n".join(body_lines).strip()

    # Handle HTML emails (convert to plain text)
    if "<html" in email_body.lower():
        soup = BeautifulSoup(email_body, "html.parser")
        email_body = soup.get_text(separator=" ", strip=True)

    return email_body

# Extract cleaned email bodies using refined method
cleaned_email_bodies_refined = {
    category: extract_email_body_refined(content) for category, content in expanded_sample_contents.items()
}

cleaned_email_bodies_refined


import quopri

def clean_final_email_text(email_body):
    """
    Cleans extracted email text by removing unnecessary metadata artifacts, 
    decoding quoted-printable text, and keeping only meaningful content.
    """
    # Decode quoted-printable encoded text (common in spam emails)
    email_body = quopri.decodestring(email_body).decode(errors="ignore")

    # Remove common email metadata fields that may persist
    email_body = re.sub(r"^(Message-Id:|X-Loop:|Sender:|Errors-To:).*", "", email_body, flags=re.MULTILINE)

    # Remove excessive spacing and blank lines
    email_body = "\n".join([line.strip() for line in email_body.splitlines() if line.strip()])

    return email_body

# Apply final cleaning to extracted email bodies
final_cleaned_emails = {
    category: clean_final_email_text(content) for category, content in cleaned_email_bodies_refined.items()
}

final_cleaned_emails


def remove_html_tags(text):
    """
    Removes HTML tags from a given text while preserving readable content.
    """
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text(separator=" ", strip=True)

# Apply HTML removal and additional normalization
final_cleaned_emails = {
    category: remove_html_tags(content) for category, content in final_cleaned_emails.items()
}

# Re-check easy_ham to ensure content is retained
if not final_cleaned_emails["easy_ham"].strip():
    # Re-extract using a more lenient approach
    final_cleaned_emails["easy_ham"] = clean_email_text(expanded_sample_contents["easy_ham"])

final_cleaned_emails



# Re-extract the easy_ham email body using a more relaxed method
def extract_easy_ham_body(email_text):
    """
    Attempts a more relaxed extraction of the body from easy_ham emails,
    ensuring that meaningful text is retained.
    """
    # Remove headers but keep the first detected meaningful section
    lines = email_text.split("\n")
    body_start = next((i for i, line in enumerate(lines) if re.match(r"^(Subject:|To:|From:)", line)), len(lines))
    body_text = "\n".join(lines[body_start:]).strip()

    return body_text

# Apply re-extraction for easy_ham
final_cleaned_emails["easy_ham"] = extract_easy_ham_body(expanded_sample_contents["easy_ham"])

# Check final output
final_cleaned_emails["easy_ham"]


def remove_email_metadata(email_text):
    """
    Removes metadata fields like 'From:', 'To:', 'Subject:', etc. from the email body.
    Retains only meaningful message content.
    """
    lines = email_text.split("\n")
    cleaned_lines = [line for line in lines if not re.match(r"^(From:|To:|Subject:|In-Reply-To:|References:|MIME-Version:|Content-Type:|Message-Id:|X-Loop:|Sender:|Errors-To:)", line)]
    return "\n".join(cleaned_lines).strip()

# Apply final metadata removal to easy_ham
final_cleaned_emails["easy_ham"] = remove_email_metadata(final_cleaned_emails["easy_ham"])

# Check the final cleaned version of easy_ham
final_cleaned_emails["easy_ham"]


#  extract message ignoring headers and metadata

def extract_message_body(file_path):
    """
    Extracts the message body from an email file, ignoring headers and metadata.
    Handles quoted-printable encoding and HTML content.
    """
    try:
        with open(file_path, "r", encoding="latin-1") as f:
            email_text = f.read()
    except FileNotFoundError:
        print(f"Error: File not found at {file_path}")
        return ""
    except Exception as e:
        print(f"An error occurred while reading '{file_path}': {e}")
        return ""
    
    # Decode quoted-printable encoding
    email_text = quopri.decodestring(email_text).decode(errors="ignore")

    # Remove headers (everything before the first blank line)
    lines = email_text.split("\n\n", 1)  # Split at the first blank line
    if len(lines) > 1:
        email_body = lines[1]
    else:
        email_body = ""
    
    # Remove HTML tags
    email_body = remove_html_tags(email_body)

    # Remove common email metadata fields that may persist
    email_body = re.sub(r"^(Message-Id:|X-Loop:|Sender:|Errors-To:).*", "", email_body, flags=re.MULTILINE)
    
    # Remove excessive spacing and blank lines
    email_body = "\n".join([line.strip() for line in email_body.splitlines() if line.strip()])

    return email_body

def remove_html_tags(text):
    """Removes HTML tags from a given text while preserving readable content."""
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text(separator=" ", strip=True)


    # show extracted messages after ifnoring headers and metadata

def extract_message_body(file_path):
    """
    Extracts the message body from an email file, ignoring headers and metadata.
    Handles quoted-printable encoding and HTML content.
    """
    try:
        with open(file_path, "r", encoding="latin-1") as f:
            email_text = f.read()
    except FileNotFoundError:
        print(f"Error: File not found at {file_path}")
        return ""
    except Exception as e:
        print(f"An error occurred while reading '{file_path}': {e}")
        return ""
    
    # Decode quoted-printable encoding
    email_text = quopri.decodestring(email_text).decode(errors="ignore")

    # Remove headers (everything before the first blank line)
    lines = email_text.split("\n\n", 1)  # Split at the first blank line
    if len(lines) > 1:
        email_body = lines[1]
    else:
        email_body = ""
    
    # Remove HTML tags
    email_body = remove_html_tags(email_body)

    # Remove common email metadata fields that may persist
    email_body = re.sub(r"^(Message-Id:|X-Loop:|Sender:|Errors-To:).*", "", email_body, flags=re.MULTILINE)
    
    # Remove excessive spacing and blank lines
    email_body = "\n".join([line.strip() for line in email_body.splitlines() if line.strip()])

    return email_body

def remove_html_tags(text):
    """Removes HTML tags from a given text while preserving readable content."""
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text(separator=" ", strip=True)

# Example usage (assuming you have a file path to an email)

# Replace with actual file paths from your extracted emails
for category, file in sample_files.items():
  file_path = os.path.join(extract_path, category, file)
  extracted_body = extract_message_body(file_path)
  print(f"Extracted message body from {file_path}:\n{extracted_body}\n{'='*50}")


!pip install nltk
import nltk

# Download the 'punkt_tab' data
nltk.download('punkt_tab')


# Remove email signatures and unsubscribe links (common in spam emails).
# Filter out advertising-style text (often seen in hard ham and spam).
# Standardize text casing and punctuation for better NLP processing.
# Tokenize and vectorize the text for machine learning.

import nltk
import re
from sklearn.feature_extraction.text import TfidfVectorizer

# Download required NLTK resources if not already present
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
  nltk.data.find('corpora/stopwords')
except LookupError:
  nltk.download('stopwords')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

def preprocess_text(text):
    """
    Preprocesses the email text:
    - Removes email signatures and unsubscribe links.
    - Filters advertising-style text.
    - Standardizes casing and punctuation.
    - Tokenizes and removes stop words.
    """

    # Remove email signatures (heuristic: look for "unsubscribe" or similar phrases)
    text = re.sub(r"unsubscribe|opt out|reply to.*", "", text, flags=re.IGNORECASE)

    # Filter advertising-style text (heuristic: look for promotional keywords)
    advertising_keywords = r"free|limited time|offer|discount|promotion|guaranteed|exclusive|click here|buy now"
    text = re.sub(advertising_keywords, "", text, flags=re.IGNORECASE)


    # Standardize casing and punctuation
    text = text.lower()
    text = re.sub(r"[^\w\s]", "", text) # Remove punctuation


    # Tokenize and remove stop words
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [w for w in tokens if not w in stop_words and w.isalnum()]

    return " ".join(filtered_tokens)


# Example usage with sample files
preprocessed_emails = {}
for category, file in sample_files.items():
  file_path = os.path.join(extract_path, category, file)
  extracted_body = extract_message_body(file_path)
  preprocessed_emails[category] = preprocess_text(extracted_body)

# Print preprocessed emails
for category, text in preprocessed_emails.items():
    print(f"Category: {category}\nPreprocessed Text:\n{text}\n{'=' * 50}")

# Vectorize the text using TF-IDF
vectorizer = TfidfVectorizer()
vectors = vectorizer.fit_transform(preprocessed_emails.values())

# Print the TF-IDF vectors (sparse matrix)
print("TF-IDF Vectors:")
vectors


import pandas as pd
import re
from bs4 import BeautifulSoup
import quopri
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

def clean_email_content(email_text):
    """
    Cleans the email content by removing unwanted artifacts such as:
    - Email signatures
    - Unsubscribe links
    - Promotional phrases
    - Excess whitespace
    """
    # Remove unsubscribe and promotional links
    email_text = re.sub(r"(unsubscribe|click here|visit our website|buy now|special offer).*", "", email_text, flags=re.IGNORECASE)
    # Remove email signatures and footer sections (heuristic approach)
    email_text = re.sub(r"--\n.*", "", email_text, flags=re.DOTALL)
    # Normalize text (convert to lowercase, remove excessive spaces)
    email_text = email_text.lower().strip()
    return email_text

# Assuming 'final_cleaned_emails' is defined from the previous code
# Apply the cleaning function 
# (replace with your actual 'final_cleaned_emails' dictionary)
final_cleaned_emails = {} # Replace this with your actual data

final_cleaned_emails = {category: clean_email_content(content) for category, content in final_cleaned_emails.items()}

# Convert extracted messages into a structured DataFrame
email_data = pd.DataFrame({
    "Category": list(final_cleaned_emails.keys()),
    "Message": list(final_cleaned_emails.values())
})

# Display the DataFrame (no more ace_tools dependency)
email_data
print({category: len(content) for category, content in final_cleaned_emails.items()})



Train and Evaluate a Naive Bayes Classifier

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import pandas as pd

# Assuming 'vectors' and 'preprocessed_emails' are defined from the previous code
# Convert dictionary to DataFrame

email_df = pd.DataFrame(list(preprocessed_emails.items()), columns=['Category', 'Message'])
# Prepare data for training
X = vectors
y = email_df['Category']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Multinomial Naive Bayes classifier
nb_classifier = MultinomialNB()
nb_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred = nb_classifier.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")
print(f"Classification Report:\n{classification_report(y_test, y_pred)}")
print(f"Confusion Matrix:\n{confusion_matrix(y_test, y_pred)}")





import pandas as pd
import numpy as np
import re
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import cross_val_score, KFold # Import KFold
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize


# Ensure necessary NLTK resources are available
nltk.download('punkt')
nltk.download('stopwords')

# Check if emails have been preprocessed
if not preprocessed_emails:
    raise ValueError("No preprocessed emails found. Ensure emails are properly extracted and cleaned.")

# Convert the preprocessed emails into a DataFrame
email_df = pd.DataFrame(list(preprocessed_emails.items()), columns=['Category', 'Message'])

# Convert categorical labels to numeric format
label_mapping = {"easy_ham": 0, "spam": 1, "hard_ham": 2}
email_df["Label"] = email_df["Category"].map(label_mapping)

# Ensure all categories have at least 2 samples
if email_df["Label"].value_counts().min() < 2:
    print("\n⚠ Warning: Some categories have fewer than 2 samples. Cross-validation is the only option.\n")

# Vectorize text using TF-IDF
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(email_df["Message"])
y = email_df["Label"]

# Train and evaluate using cross-validation with KFold
nb_classifier = MultinomialNB()
# Use KFold with 3 splits and shuffle the data
cv = KFold(n_splits=min(3, len(email_df)), shuffle=True, random_state=42)  
cv_scores = cross_val_score(nb_classifier, X, y, cv=cv, scoring="accuracy") 

# Print cross-validation results
print(f"\n📊 Cross-Validation Accuracy Scores: {cv_scores}")
print(f"📈 Mean Accuracy: {cv_scores.mean():.4f}")



# with train test  boo
import pandas as pd
import numpy as np
import re
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Ensure necessary NLTK resources are available
nltk.download('punkt')
nltk.download('stopwords')

# Sample emails should have been preprocessed into `preprocessed_emails`
email_df = pd.DataFrame(list(preprocessed_emails.items()), columns=['Category', 'Message'])

# Convert categorical labels to numeric format
label_mapping = {"easy_ham": 0, "spam": 1, "hard_ham": 2}
email_df["Label"] = email_df["Category"].map(label_mapping)

# Vectorize text using TF-IDF
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(email_df["Message"])
y = email_df["Label"]

# Check if any class has only one sample
class_counts = np.bincount(y)
if np.any(class_counts < 2):
    print("Warning: Some classes have less than 2 samples. Stratified split might not be possible.")
    # You can either:
    # 1. Remove classes with less than 2 samples
    # 2. Use a different splitting strategy (e.g., without stratification)

    # Here, we'll proceed without stratification if any class has less than 2 samples:
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) 
else:
    # Perform stratified split if all classes have at least 2 samples
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Train Naive Bayes classifier
nb_classifier = MultinomialNB()
nb_classifier.fit(X_train, y_train)

# Make predictions
y_pred = nb_classifier.predict(X_test)

# Evaluate model performance
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

# Get unique labels present in the training and test sets
unique_labels_train = np.unique(y_train)
unique_labels_test = np.unique(y_test)

# Get the labels that are present in both training and test sets
common_labels = np.intersect1d(unique_labels_train, unique_labels_test)

# Filter label_mapping to include only common labels
filtered_label_mapping = {k: v for k, v in label_mapping.items() if v in common_labels}

# Generate classification report with target names for common labels only
# If no common labels, use the labels from the training set
if len(filtered_label_mapping) > 0:
    class_report = classification_report(y_test, y_pred, target_names=list(filtered_label_mapping.keys()))
else:
    filtered_label_mapping = {k: v for k, v in label_mapping.items() if v in unique_labels_train}
    class_report = classification_report(y_test, y_pred, target_names=list(filtered_label_mapping.keys()))
    print("Warning: No common labels between training and test sets. Using training set labels for classification report.")



# Display results
print(f"Naive Bayes Model Accuracy: {accuracy:.4f}")
print("\nConfusion Matrix:\n", conf_matrix)
print("\nClassification Report:\n", class_report)


### cut
// from sklearn.model_selection import GridSearchCV
// import numpy as np

// # Define expanded hyperparameter grid for alpha
// param_grid = {"alpha": np.logspace(-6, 6, 20)}  # Expanding search range

// # Initialize Naïve Bayes classifier
// nb_classifier = MultinomialNB()

// # Perform Grid Search with Cross-Validation
// grid_search = GridSearchCV(nb_classifier, param_grid, cv=5, scoring="accuracy")  # Increase cross-validation folds
// grid_search.fit(X, y)

// # Get best parameters and best score
// best_alpha = grid_search.best_params_["alpha"]
// best_score = grid_search.best_score_

// print(f"Best Alpha: {best_alpha}")
// print(f"Best Accuracy: {best_score:.4f}")
