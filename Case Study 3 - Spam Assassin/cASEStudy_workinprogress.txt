ok.  lets  tackle case study 3 - on spam assassin: SPAM ASSIN CASE STUDY

1. 
Welcome to our case study on spam. This week, we'll be dealing with an issue of a classifier. And our data is no longer contained in an ICE data frame or spreadsheet. An important part of this will be processing the data.
Follow along the video as you get more details about the problem. As always, see if you have the same or different questions as our protagonist about what's going on with the data and the details of the problem. This week, we'll be probably using some clustering. And you definitely want to use a naive base for your assignment.
Good luck and enjoy the video coming up. And remember, as always, to add your questions to your upload for your participation.

2. 
OK, so this week, we received a request from our IT department in that we as a company receive a significant amount of spam. So we've been given a collection of emails, both normal and spam, and our goal is that we need to process this data and then build a classifier that will classify emails as spam or not spam. So our goal for this week is to build a model that can take any given email and provide a classification. So essentially, what we're doing is we're building an email filter. And your task is to go through all of this data, process it, and then build, based on the contents of an email, a model that can predict whether any given email is spam or not spam. So that's your assignment. Are there any questions?

3. 
OK, so now that you've had a chance to look over the actual emails, are there any questions? Yeah, I actually have one overarching question. So what happens if we misclassify something as spam when it's not or vice versa? Would you rather us filter out too many spam messages and risk moving real emails to the Spam folder, or would you rather have a few spam messages make it through so that the real emails don't get lost?
Yeah, that's an excellent question. And, I mean, I leave that up to you. Obviously, nobody wants to lose an important email by having it go into the Spam folder, but we're losing important emails now because they're getting lost in the shuffle because of the sheer volume of spam. So really it's whatever you decide works best.
That's your call. OK, got it. So I do have actually one more question. I know I said I had one, but I have another one.
My other question is about the data set. Is there a spreadsheet or file of this data? Yeah. Unfortunately, there is not.
All we have are the emails themselves. So you are going to have to generate your own data set for this assignment. OK. But this is definitely something a little new for us.
Is there anything that's been done on spam filtering before that we can check out for this? I believe that in the past, people have used Naive Bayes as an email filter. And it's been quite successful. So I would look into that.
OK. Great.

Case Study 3.
Build a spam classifier using naive Bayes and clustering. You will have to create your own dataset from the input messages. Be sure to document how you created your dataset.

sample for captions: 
arxiv.org/pdf/2502.05078


Build a spam classifier using naive Bayes and clustering. You will have to create your own dataset from the input messages. Be sure to document how you created your dataset.


Feedback from case study 2: 

Feb 10 at 7pm
Lets make sure we get actual tables for the classification report and confusion matrix (or use ConfusionMatrixDisplay.frompredictions()

Definitly need a wider value search for C, for instance I do 10E-6 to 10R+6 (20 values).

Reminder are results should be Cross Validated Results so your support should be 101k for this case study

- Robert Slater



requirments for all case studies: 


Case Study Requirements:
• All data prep described
o Imputation methods
▪ Why did you choose that?
o Size of data (features, examples)
o How splits were performed
• Data Science Model(s) used
o Configuration
▪ Early Stopping
▪ Loss Function/Metric Optimized
▪ Other metrics monitored
o Hyperparameters tested
▪ (aka Ablation Study)
▪ Random/Grid/Other/Combo
o Best Hyper Parameters
• Results:
o Continuous Problem
▪ Loss metrics
▪ Residuals
▪ Plots of Predicted vs Target (All examples)
o Classification Problem
▪ Confusion Matrix (all examples)
▪ Classification Report (all examples)
▪ AUC-ROC
• Binary Problems Only
▪ Precision, Recall, Sensitivity, Specificity
• NO MORE
o “Performed Well”
▪ Results speak for themselves: Accuracy was X, Precision was Y
o “Good Results”
▪ See above
o “Impressive”
▪ Unless its state of the art, its not impressive and since NONE of the
homework is on standard data, you will never have state of the art
in this class.
o DO NOT Train/Test Splits
▪ INSTEAD Cross Validate EVERYTHING



# Ensure NLTK resources are available
!pip install nltk # Install NLTK if not already installed
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab') # Download 'punkt_tab' data

#  mount google drive

from google.colab import drive
drive.mount('/content/drive')




import os
import zipfile

# Path to the uploaded zip file
zip_file_path = "/content/drive/MyDrive/SpamAssassinMessages.zip"
extract_path = "content/drive/MyDrive/extracted_files"

# Extract the ZIP file
if not os.path.exists(extract_path):
    os.makedirs(extract_path)

try:
    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
        zip_ref.extractall(extract_path)
        extracted_files = os.listdir(extract_path)
except Exception as e:
    extracted_files = []
    print(f"Error extracting files: {e}")

# Display the extracted folder structure
extracted_files

# List extracted files to examine the dataset structure
extracted_files = os.listdir("content/drive/MyDrive/extracted_files")
print(extracted_files[:10])  # Display first 10 files for inspection


# Select a sample file from each category to inspect
sample_files = {}
for category in extracted_files:
    category_path = os.path.join(extract_path, category)
    if os.path.isdir(category_path):
        files = os.listdir(category_path)
        if files:
            sample_files[category] = files[0]  # Select the first file in the category

# Read the first few lines of each sample file
sample_contents = {}
for category, file in sample_files.items():
    file_path = os.path.join(extract_path, category, file)
    try:
        with open(file_path, "r", encoding="latin-1") as f:
            sample_contents[category] = "".join(f.readlines()[:15])  # Read first 15 lines
    except Exception as e:
        sample_contents[category] = f"Error reading file: {e}"

# Display sample contents
sample_contents


import re
import quopri
from bs4 import BeautifulSoup

def extract_email_body(email_text):
    """
    Extracts the body of an email by removing headers and decoding any encoded text.
    Handles quoted-printable encoding and removes HTML content.
    """
    # Decode quoted-printable encoding
    email_text = quopri.decodestring(email_text).decode(errors="ignore")

    # Remove headers (everything before the first blank line)
    lines = email_text.split("\n\n", 1)  # Split at the first blank line
    email_body = lines[1] if len(lines) > 1 else email_text

    # Remove HTML tags if the email contains HTML
    soup = BeautifulSoup(email_body, "html.parser")
    email_body = soup.get_text(separator=" ", strip=True)

    # Remove metadata fields that may persist
    email_body = re.sub(r"^(Message-Id:|X-Loop:|Sender:|Errors-To:).*", "", email_body, flags=re.MULTILINE)

    # Remove excessive whitespace and blank lines
    email_body = "\n".join([line.strip() for line in email_body.splitlines() if line.strip()])

    return email_body

# Apply extraction to each email category
cleaned_emails = {}
for category, file in sample_files.items():
    file_path = os.path.join(extract_path, category, file)
    try:
        with open(file_path, "r", encoding="latin-1") as f:
            email_text = f.read()
            cleaned_emails[category] = extract_email_body(email_text)
    except Exception as e:
        cleaned_emails[category] = f"Error reading file: {e}"

# Display extracted email bodies
cleaned_emails

import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Ensure necessary NLTK resources are available
nltk.download('punkt')
nltk.download('stopwords')

# Function to preprocess the email text
def preprocess_email_text(text):
    """
    Preprocesses the email text by:
    - Removing unnecessary metadata and spam-specific artifacts.
    - Standardizing text casing and punctuation.
    - Removing stopwords.
    - Tokenizing text.
    """

    # Remove common promotional phrases in spam
    text = re.sub(r"(unsubscribe|click here|buy now|special offer|limited time|free trial|guaranteed).*", "", text, flags=re.IGNORECASE)

    # Remove excessive spaces and newlines
    text = re.sub(r"\s+", " ", text).strip()

    # Convert text to lowercase
    text = text.lower()

    # Remove punctuation and numbers
    text = re.sub(r"[^\w\s]", "", text)

    # Tokenize and remove stop words
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words("english"))
    filtered_tokens = [word for word in tokens if word not in stop_words]

    return " ".join(filtered_tokens)

# Apply preprocessing to all extracted emails
preprocessed_emails = {category: preprocess_email_text(content) for category, content in cleaned_emails.items()}

# Display a sample of the cleaned emails
preprocessed_emails

import pandas as pd # Import the pandas library and assign it to the name 'pd'

from sklearn.model_selection import cross_val_score, KFold
from sklearn.naive_bayes import MultinomialNB

# Assign labels: 0 for Ham (Non-Spam), 1 for Spam
label_mapping = {
    "easy_ham": 0,
    "easy_ham_2": 0,
    "hard_ham": 0,
    "spam": 1,
    "spam_2": 1
}

# Convert dataset into a structured format
email_df = pd.DataFrame({ # Now 'pd' is recognized as pandas
    "Category": list(preprocessed_emails.keys()),
    "Message": list(preprocessed_emails.values()),
    "Label": [label_mapping[category] for category in preprocessed_emails.keys()]
})


from sklearn.model_selection import cross_val_score, KFold
from sklearn.naive_bayes import MultinomialNB

# Assign labels: 0 for Ham (Non-Spam), 1 for Spam
label_mapping = {
    "easy_ham": 0,
    "easy_ham_2": 0,
    "hard_ham": 0,
    "spam": 1,
    "spam_2": 1
}

# Convert dataset into a structured format
email_df = pd.DataFrame({
    "Category": list(preprocessed_emails.keys()),
    "Message": list(preprocessed_emails.values()),
    "Label": [label_mapping[category] for category in preprocessed_emails.keys()]
})

# Vectorize the text using TF-IDF
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(email_df["Message"])
y = email_df["Label"]

# Initialize Naïve Bayes classifier
nb_classifier = MultinomialNB()

# Perform **cross-validation** using KFold (3 splits, shuffled data)
cv = KFold(n_splits=3, shuffle=True, random_state=42)
cv_scores = cross_val_score(nb_classifier, X, y, cv=cv, scoring="accuracy")

# Display cross-validation accuracy results
cv_scores.mean(), cv_scores

# Check class distribution
email_df["Label"].value_counts()


from sklearn.model_selection import GridSearchCV

# Define hyperparameter grid for alpha (Laplace smoothing)
param_grid = {"alpha": [0.01, 0.1, 0.5, 1, 5, 10]}

# Initialize Naïve Bayes classifier
nb_classifier = MultinomialNB()

# Perform Grid Search with Cross-Validation
grid_search = GridSearchCV(nb_classifier, param_grid, cv=3, scoring="accuracy")
grid_search.fit(X, y)

# Get best parameters and best score
best_alpha = grid_search.best_params_["alpha"]
best_score = grid_search.best_score_

best_alpha, best_score


from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

# Train final model using best alpha
final_nb = MultinomialNB(alpha=best_alpha)
final_nb.fit(X, y)

# Predict using cross-validation
y_pred = cross_val_score(final_nb, X, y, cv=3)

# Generate classification report
class_report = classification_report(y, final_nb.predict(X), target_names=["Ham", "Spam"])

# Generate confusion matrix
conf_matrix = confusion_matrix(y, final_nb.predict(X))

# Display confusion matrix
disp = ConfusionMatrixDisplay(conf_matrix, display_labels=["Ham", "Spam"])
disp.plot()

# Output classification report and confusion matrix
class_report, conf_matrix

import numpy as np # Import the numpy library and assign it to the alias 'np'
np.logspace(-6, 6, 20)  # 20 values between 10^-6 and 10^6

































Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)
Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)
Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)
Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)
Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)
[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
[nltk_data] Downloading package punkt_tab to /root/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount("/content/drive", force_remount=True).
['easy_ham_2', 'hard_ham', 'easy_ham', 'spam', 'spam_2']
[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.
  warnings.warn(
/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.
  warnings.warn(
('              precision    recall  f1-score   support\n\n         Ham       1.00      1.00      1.00         3\n        Spam       1.00      1.00      1.00         2\n\n    accuracy                           1.00         5\n   macro avg       1.00      1.00      1.00         5\nweighted avg       1.00      1.00      1.00         5\n',
 array([[3, 0],
        [0, 2]]))
















































next steps?  generate synthetic spam data or apply clustering as the case study suggests? ​​
what else?  consisder his feedback from last week, the class transcript with instriuctions. and the requirements. 