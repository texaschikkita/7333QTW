# 1. eda

import pandas as pd

# Load datasets
diabetic_data_path = r"D:\7333\Case Study 2\diabetic_data.csv"
ids_mapping_path = r"D:\7333\Case Study 2\IDs_mapping.csv"

diabetic_data = pd.read_csv(diabetic_data_path)
ids_mapping = pd.read_csv(ids_mapping_path)

# Replace '?' with NaN for proper missing value handling
diabetic_data.replace('?', pd.NA, inplace=True)

# Display dataset info
print("\nðŸ“Š Diabetic Data Info:")
print(diabetic_data.info())

print("\nðŸ” First few rows of diabetic_data:")
print(diabetic_data.head())

print("\nðŸ“Š IDs Mapping Data Info:")
print(ids_mapping.info())

print("\nðŸ” First few rows of IDs_mapping:")
print(ids_mapping.head())

# Check missing values
print("\nâš ï¸ Missing values in dataset:")
print(diabetic_data.isnull().sum()[diabetic_data.isnull().sum() > 0])



# 2 Handle Missing Values

import numpy as np

# Drop columns with excessive missing values
columns_to_drop = ['weight', 'payer_code', 'medical_specialty']
diabetic_data.drop(columns=columns_to_drop, inplace=True)

# Fill missing categorical values
diabetic_data.fillna({
    'race': 'Unknown',
    'diag_1': 'Missing',
    'diag_2': 'Missing',
    'diag_3': 'Missing',
    'max_glu_serum': 'None',
    'A1Cresult': 'None'
}, inplace=True)

# Verify missing values are handled
print("\nâœ… Missing values after handling:")
print(diabetic_data.isnull().sum()[diabetic_data.isnull().sum() > 0])


# 3 Merge IDs Mapping dataset

# Ensure 'admission_type_id' is the same data type in both datasets
diabetic_data['admission_type_id'] = diabetic_data['admission_type_id'].astype(str)
ids_mapping['admission_type_id'] = ids_mapping['admission_type_id'].astype(str)

# Merge IDs mapping with diabetic_data (if applicable)
diabetic_data = diabetic_data.merge(ids_mapping, on='admission_type_id', how='left')

# Verify the merge
print("\nâœ… First few rows after merging:")
print(diabetic_data.head())


# 4 Encode Categorivcal Values

from sklearn.preprocessing import LabelEncoder

# Ensure 'readmitted' is mapped correctly
readmitted_mapping = {'NO': 0, '>30': 1, '<30': 2}
diabetic_data['readmitted'] = diabetic_data['readmitted'].map(readmitted_mapping)

# Identify categorical columns
categorical_cols = ['race', 'gender', 'age', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']
medication_cols = ['metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride',
                   'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone',
                   'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide',
                   'examide', 'citoglipton', 'insulin', 'glyburide-metformin', 
                   'glipizide-metformin', 'glimepiride-pioglitazone', 'metformin-rosiglitazone',
                   'metformin-pioglitazone']

# Include diagnosis codes as categorical
all_categorical = categorical_cols + medication_cols + ['diag_1', 'diag_2', 'diag_3']

# Apply Label Encoding
label_encoders = {}
for col in all_categorical:
    if diabetic_data[col].dtype == "object":  # Only encode object columns
        le = LabelEncoder()
        diabetic_data[col] = le.fit_transform(diabetic_data[col].astype(str))
        label_encoders[col] = le  # Save encoders for possible inverse transformation

# Verify all columns are now numerical
print("\nâœ… Updated column data types (should all be numeric now):")
print(diabetic_data.dtypes)


# 5 Fix Data Types Before Train Test Split

# Convert 'admission_type_id' to numeric (it should not be object)
diabetic_data['admission_type_id'] = diabetic_data['admission_type_id'].astype(int)

# Drop 'description' column (it's not useful for modeling)
diabetic_data.drop(columns=['description'], inplace=True)

# Confirm all columns are now numeric
print("\nâœ… Updated column data types (ALL should be numeric now):")
print(diabetic_data.dtypes)


#6. Train test split and Scaling

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Drop ID columns (not useful for predictions)
X = diabetic_data.drop(columns=['readmitted', 'encounter_id', 'patient_nbr'])
y = diabetic_data['readmitted']

# Split into train (80%) and test (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Standardize numerical features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Confirm split sizes
print("\nâœ… Training set size:", X_train.shape)
print("âœ… Test set size:", X_test.shape)


# 7. Train ML model

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Initialize and train logistic regression model
model = LogisticRegression(max_iter=500, random_state=42)
model.fit(X_train_scaled, y_train)

# Make predictions
y_pred = model.predict(X_test_scaled)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"\nâœ… Model Accuracy: {accuracy:.4f}")

# Display classification report
print("\nðŸ“Š Classification Report:")
print(classification_report(y_test, y_pred))




# 8 Improve modeling
# 8a m# 8 Improve the performance
# 8A HANDLE CLASS IMBALANCE

from imblearn.over_sampling import SMOTE
from collections import Counter

# Print class distribution before balancing
print("\nðŸ” Class distribution before SMOTE:", Counter(y_train))

# Apply SMOTE (Synthetic Minority Over-sampling Technique)
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)

# Print class distribution after balancing
print("\nâœ… Class distribution after SMOTE:", Counter(y_train_resampled))


from imblearn.over_sampling import SMOTE

# Apply SMOTE to balance the training dataset
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Check the new class distribution
from collections import Counter
print("\nðŸ” Class distribution after SMOTE:", Counter(y_train_smote))
 

# 8b retrain

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Retrain the model on the balanced dataset
rf_balanced = RandomForestClassifier(random_state=42, n_jobs=-1)
rf_balanced.fit(X_train_smote, y_train_smote)

# Predictions on test set
y_pred_balanced = rf_balanced.predict(X_test)

# Evaluate performance
accuracy_balanced = accuracy_score(y_test, y_pred_balanced)
print(f"\nâœ… Model Accuracy After SMOTE: {accuracy_balanced:.4f}")

# Print classification report
print("\nðŸ“Š Classification Report After SMOTE:")
print(classification_report(y_test, y_pred_balanced))





```{python}
# 1. eda

import pandas as pd

# Load datasets
diabetic_data_path = r"D:\7333\Case Study 2\diabetic_data.csv"
ids_mapping_path = r"D:\7333\Case Study 2\IDs_mapping.csv"

diabetic_data = pd.read_csv(diabetic_data_path)
ids_mapping = pd.read_csv(ids_mapping_path)

# Replace '?' with NaN for proper missing value handling
diabetic_data.replace('?', pd.NA, inplace=True)

# Display dataset info
print("\nðŸ“Š Diabetic Data Info:")
print(diabetic_data.info())

print("\nðŸ” First few rows of diabetic_data:")
print(diabetic_data.head())

print("\nðŸ“Š IDs Mapping Data Info:")
print(ids_mapping.info())

print("\nðŸ” First few rows of IDs_mapping:")
print(ids_mapping.head())

# Check missing values
print("\nâš ï¸ Missing values in dataset:")
print(diabetic_data.isnull().sum()[diabetic_data.isnull().sum() > 0])



# 2 Handle Missing Values

import numpy as np

# Drop columns with excessive missing values
columns_to_drop = ['weight', 'payer_code', 'medical_specialty']
diabetic_data.drop(columns=columns_to_drop, inplace=True)

# Fill missing categorical values
diabetic_data.fillna({
    'race': 'Unknown',
    'diag_1': 'Missing',
    'diag_2': 'Missing',
    'diag_3': 'Missing',
    'max_glu_serum': 'None',
    'A1Cresult': 'None'
}, inplace=True)

# Verify missing values are handled
print("\nâœ… Missing values after handling:")
print(diabetic_data.isnull().sum()[diabetic_data.isnull().sum() > 0])


# 3 Merge IDs Mapping dataset

# Ensure 'admission_type_id' is the same data type in both datasets
diabetic_data['admission_type_id'] = diabetic_data['admission_type_id'].astype(str)
ids_mapping['admission_type_id'] = ids_mapping['admission_type_id'].astype(str)

# Merge IDs mapping with diabetic_data (if applicable)
diabetic_data = diabetic_data.merge(ids_mapping, on='admission_type_id', how='left')

# Verify the merge
print("\nâœ… First few rows after merging:")
print(diabetic_data.head())


# 4 Encode Categorivcal Values

from sklearn.preprocessing import LabelEncoder

# Ensure 'readmitted' is mapped correctly
readmitted_mapping = {'NO': 0, '>30': 1, '<30': 2}
diabetic_data['readmitted'] = diabetic_data['readmitted'].map(readmitted_mapping)

# Identify categorical columns
categorical_cols = ['race', 'gender', 'age', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']
medication_cols = ['metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride',
                   'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone',
                   'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide',
                   'examide', 'citoglipton', 'insulin', 'glyburide-metformin', 
                   'glipizide-metformin', 'glimepiride-pioglitazone', 'metformin-rosiglitazone',
                   'metformin-pioglitazone']

# Include diagnosis codes as categorical
all_categorical = categorical_cols + medication_cols + ['diag_1', 'diag_2', 'diag_3']

# Apply Label Encoding
label_encoders = {}
for col in all_categorical:
    if diabetic_data[col].dtype == "object":  # Only encode object columns
        le = LabelEncoder()
        diabetic_data[col] = le.fit_transform(diabetic_data[col].astype(str))
        label_encoders[col] = le  # Save encoders for possible inverse transformation

# Verify all columns are now numerical
print("\nâœ… Updated column data types (should all be numeric now):")
print(diabetic_data.dtypes)


# 5 Fix Data Types Before Train Test Split

# Convert 'admission_type_id' to numeric (it should not be object)
diabetic_data['admission_type_id'] = diabetic_data['admission_type_id'].astype(int)

# Drop 'description' column (it's not useful for modeling)
diabetic_data.drop(columns=['description'], inplace=True)

# Confirm all columns are now numeric
print("\nâœ… Updated column data types (ALL should be numeric now):")
print(diabetic_data.dtypes)


#6. Train test split and Scaling

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Drop ID columns (not useful for predictions)
X = diabetic_data.drop(columns=['readmitted', 'encounter_id', 'patient_nbr'])
y = diabetic_data['readmitted']

# Split into train (80%) and test (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Standardize numerical features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Confirm split sizes
print("\nâœ… Training set size:", X_train.shape)
print("âœ… Test set size:", X_test.shape)


# 7. Train ML model

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Initialize and train logistic regression model
model = LogisticRegression(max_iter=500, random_state=42)
model.fit(X_train_scaled, y_train)

# Make predictions
y_pred = model.predict(X_test_scaled)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"\nâœ… Model Accuracy: {accuracy:.4f}")

# Display classification report
print("\nðŸ“Š Classification Report:")
print(classification_report(y_test, y_pred))




# 8 Improve modeling
# 8a m# 8 Improve the performance
# 8A HANDLE CLASS IMBALANCE

from imblearn.over_sampling import SMOTE
from collections import Counter

# Print class distribution before balancing
print("\nðŸ” Class distribution before SMOTE:", Counter(y_train))

# Apply SMOTE (Synthetic Minority Over-sampling Technique)
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)

# Print class distribution after balancing
print("\nâœ… Class distribution after SMOTE:", Counter(y_train_resampled))


from imblearn.over_sampling import SMOTE

# Apply SMOTE to balance the training dataset
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Check the new class distribution
from collections import Counter
print("\nðŸ” Class distribution after SMOTE:", Counter(y_train_smote))
 

# 8b retrain

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Retrain the model on the balanced dataset
rf_balanced = RandomForestClassifier(random_state=42, n_jobs=-1)
rf_balanced.fit(X_train_smote, y_train_smote)

# Predictions on test set
y_pred_balanced = rf_balanced.predict(X_test)

# Evaluate performance
accuracy_balanced = accuracy_score(y_test, y_pred_balanced)
print(f"\nâœ… Model Accuracy After SMOTE: {accuracy_balanced:.4f}")

# Print classification report
print("\nðŸ“Š Classification Report After SMOTE:")
print(classification_report(y_test, y_pred_balanced))
```